{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "PATH = './cifar_net.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We load our MNIST dataset using the `torchvision` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait for both datasets to be downloaded and verified.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Examples:\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024 * 6\n",
    "dataset = Loader(batch_size=batch_size)\n",
    "trainloader = dataset.trainloader\n",
    "\n",
    "def examples(trainloader):\n",
    "    # get some random training images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, _ = dataiter.next()\n",
    "\n",
    "    # show images\n",
    "    imshow(torchvision.utils.make_grid(images[:4]))\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Examples:')\n",
    "# examples(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "An autoencoder is a type of neural network that finds the function mapping the features x to itself. This objective is known as reconstruction, and an autoencoder accomplishes this through the following process: (1) an encoder learns the data representation in lower-dimension space, i.e. extracting the most salient features of the data, and (2) a decoder learns to reconstruct the original data based on the learned representation by the encoder.\n",
    "\n",
    "We define our autoencoder class with fully connected layers for both its encoder and decoder components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using our defined autoencoder class, we have the following things to do:\n",
    "    1. We configure which device we want to run on.\n",
    "    2. We instantiate an `AE` object.\n",
    "    3. We define our optimizer.\n",
    "    4. We define our reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): ConvTranspose2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose2d(16, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(6, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*******************************************************\n",
    "****** skip this step if you dont want to train *******\n",
    "*******************************************************\n",
    "\"\"\"\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "net = AE().to(device)\n",
    "print('Network:', net)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our autoencoder for our specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1 of 120, Train Loss: 0.254  aprox. 29min left\n",
      "Epoch 2 of 120, Train Loss: 0.238  aprox. 29min left\n",
      "Epoch 3 of 120, Train Loss: 0.229  aprox. 29min left\n",
      "Epoch 4 of 120, Train Loss: 0.219  aprox. 28min left\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*******************************************************\n",
    "****** skip this step if you dont want to train *******\n",
    "*******************************************************\n",
    "\"\"\"\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "# # to train loaded network further\n",
    "# net = AE()\n",
    "# net.load_state_dict(torch.load(PATH))\n",
    "# net.to(device)\n",
    "\n",
    "def train(net, trainloader, NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            img, _ = data\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(img)\n",
    "            loss = criterion(outputs, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        timePerEpoch = (time.time() - start_time)/(epoch + 1)\n",
    "        left = timePerEpoch * (NUM_EPOCHS - epoch + 1)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'.format(epoch+1, NUM_EPOCHS, loss), ' aprox. ' + str(round(left / 60)) + ' min left')\n",
    " \n",
    "    print('Finished Training')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('final duration: ' + str(round(elapsed_time / 60)) + 'min')\n",
    "    return train_loss\n",
    "\n",
    "torch.save(net.state_dict(), PATH) \n",
    "loss = train(net, dataset.trainloader, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*******************************************************\n",
    "****** skip this step if you dont want to train *******\n",
    "*******************************************************\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss)\n",
    "ax.set(xlabel='epoch', ylabel='loss')\n",
    "ax.grid()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from eval import *\n",
    "# load test set\n",
    "dataset_iter = iter(dataset.testloader)\n",
    "images, _ = dataset_iter.next()\n",
    "length = len(images)\n",
    "\n",
    "# load network\n",
    "net2 = AE()\n",
    "net2.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# model should output latent space and not reconstruction\n",
    "net2.getLatentSpace(True)\n",
    "\n",
    "# process images\n",
    "outputs = net2(images).data.numpy()\n",
    "outputShape = np.shape(outputs)\n",
    "outputs = outputs.flatten().reshape(length, np.prod(outputShape[1:]))\n",
    "\n",
    "# get 20 closest image sets\n",
    "for i in range(20):\n",
    "    closest = findNClosest(outputs[round(random() * length)], outputs, 6)\n",
    "    closeImages = []\n",
    "    for img in closest:\n",
    "        closeImages.append(images[int(img[-1])])\n",
    "    print(i+1, ':')\n",
    "    imshow(torchvision.utils.make_grid(closeImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
